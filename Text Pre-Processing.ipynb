{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Student Name: Rajath Akshay Vanikul\n",
    "#### Student ID: 29498724\n",
    "\n",
    "Date: 14/03/2019\n",
    "\n",
    "Version: 2.0\n",
    "\n",
    "Environment: Python 3.6.4 and Anaconda 5.7.6 (64-bit)\n",
    "\n",
    "Libraries used: \n",
    "* re 2.2.1 (for regular expression, included in Anaconda Python 3.6) \n",
    "* nltk 3.2.2 (Natural Language Toolkit, included in Anaconda Python 3.6)\n",
    "* itertools 2.3 (iterator building blocks,included in Anaconda Python 3.6)\n",
    "* nltk.collocations (for finding bigrams, included in Anaconda Python 3.6)\n",
    "* nltk.tokenize (for tokenization, included in Anaconda Python 3.6)\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "The task is to extract data into a proper format from a PDF file. The pdf file contains a table in which each row contains information about a unit which is unit code, synopsis, and outcomes. I will have to extract and\n",
    "transform the information for each unit into a vector space model.\n",
    "\n",
    "Methodology used to perform this task is as follows:\n",
    "\n",
    "1. Extract the information from PDF file to a TEXT file. (Used https://pdftotext.com/) \n",
    "2. Text is normalized to lowercase except the capital tokens appeared in the middle of a sentence/line\n",
    "3. Perform word tokenization using regular expression, `\\w+(?:[-']\\w+)?`\n",
    "4. The context-independent and context-dependent stop words are removed from the vocab. The stop words file is provided. (i.e, stopwords_en.txt)\n",
    "5. Tokens with the length less than 3 should be removed from the vocab.\n",
    "6. First 200 meaningful bigrams (i.e., collocations) are determined using PMI measure and included in vocab. This should be done after removal of stopwords to ensure elimination of unnecessary bigrams of stopwords.\n",
    "7. Tokens should be stemmed using the Porter stemmer. Stemming must be performed after bigrams to ensure least loss of information.\n",
    "8. Rare tokens (with the threshold set to %5) must be removed from the vocab.\n",
    "9. Find the set of all the unique vocab, index and sort them in alphabetical order.\n",
    "10. Compare each document(each unit observation) and create a Sparse Matrix.\n",
    "\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries\n",
    "\n",
    "importing regular expression, nltk and itertools to perform tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.collocations import *\n",
    "from itertools import chain\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting and loading the data\n",
    "\n",
    "I have used an online platform to convert my PDF file to a text format file. I uploaded the PDF to [https://pdftotext.com/] which helped me to convert the file to text. I downloaded the text file as `pdftotext.txt`.\n",
    "We will be using this text file for the rest of the tasks.\n",
    "\n",
    "The following code will help me read the complete text file to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title\\n',\n",
       " '\\n',\n",
       " 'Synopsis\\n',\n",
       " '\\n',\n",
       " 'Outcomes\\n',\n",
       " '\\n',\n",
       " 'ATS3221\\n',\n",
       " '\\n',\n",
       " 'In this unit students consider the central production,\\n',\n",
       " 'consumption and policy contexts of popular music.\\n',\n",
       " 'The unit examines how popular music remains a\\n',\n",
       " 'significant media and cultural industry in the\\n',\n",
       " 'production of content and meaning. It assesses the\\n',\n",
       " 'core music-media output across print, broadcasting,\\n',\n",
       " 'mobile media, film, internet and related media\\n',\n",
       " 'industries. The unit also looks at how government\\n',\n",
       " 'policy shapes music production and consumption, and\\n',\n",
       " 'how local music-making and listening is shaped by\\n',\n",
       " 'global media practices. This includes examination of\\n',\n",
       " 'key debates about music-media technologies,\\n',\n",
       " 'intellectual property frameworks and the impact of\\n',\n",
       " 'music across different media content.\\n',\n",
       " '\\n',\n",
       " \"['discuss key media studies, popular music and\\n\",\n",
       " 'cultural studies theories associated with popular\\n',\n",
       " \"music activity;', 'assess how popular music operates\\n\",\n",
       " \"as part of local and global media industries;', 'critically\\n\",\n",
       " 'and independently engage with key debates and\\n',\n",
       " \"issues within the popular music industries;', 'engage\\n\",\n",
       " \"with music industry stakeholders;', 'explain and\\n\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data = open(\"pdftotext.txt\", 'r') # read the file into the variable.\n",
    "\n",
    "#reading the first 30 lines of the file\n",
    "[next(pdf_data) for x in range(30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting the data from text file\n",
    "\n",
    "Inspecting the file, We can notice the title, synopsis and outcome pattern.\n",
    "Title, synopsis and outcome of a unit are in a sequence saperated by `\\n` as a line. If we carefully look at the complete data set. The unit title appears exactly after 3 lines of `\\n`. Similarly, we see the same pattern with synopsis and outcome with exists after 3 lines of `\\n`.\n",
    "\n",
    "We perform:\n",
    "1. Extract all the unit tiles in the text using a pattern.\n",
    "2. Extract Synopsis and outcome using the same logic and pattern recognition.\n",
    "3. Normalise the first letter of sentence in synopsis and outcome. \n",
    "4. Zip Synopsis list and outcome list together to form text value for a unit title.\n",
    "5. Create a dictionary with unit title as a key and zipped text as value.\n",
    "\n",
    "I have written a code to capture all the titles, synopsis and outcome in three different lists respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extracting unit titles\n",
    "\n",
    "As inspected, we know that the unit title appears exactly after 3 lines of \\n. I have laverages this and written a code to capture all the unit codes.\n",
    "\n",
    "Below is a code to extract all the unit titles in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the list: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ATS3221', 'APG5401', 'DIS2907', 'BFF3331', 'AMU1326', 'RAD4501']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data = open(\"pdftotext.txt\", 'r') # read the file into the variable.\n",
    "\n",
    "# initiating the re pattern object to capture the unit title.\n",
    "reg = re.compile(r\"([A-Z]+[0-9]+)\")\n",
    "# initiate a tile list to hold values.\n",
    "title_lst=[]\n",
    "\n",
    "# initiating a flag used to manupulate printing.\n",
    "line_check = False\n",
    "\n",
    "# initiating a count to keep track of \"\\n\" lines.\n",
    "count = 0\n",
    "\n",
    "# iterating through the complete text.\n",
    "for line in pdf_data:\n",
    "    \n",
    "    # search for unit title pattern.\n",
    "    match_line = re.search(r\"([A-Z]+[0-9]+)|$\",line)[1]\n",
    "    \n",
    "    # used to count the number of \"\\n\" lines.\n",
    "    if line == '\\n':\n",
    "        count += 1  \n",
    "    \n",
    "    # when the count is multiples of 3, turn the flag on.\n",
    "    if count%3 == 0:\n",
    "        line_check = True\n",
    "    \n",
    "    # when the flag is on and the unit title is found, append the result to the list.\n",
    "    if line_check and reg.match(str(match_line)):\n",
    "        title_lst.append(match_line)\n",
    "        line_check = False # switch the flag off.\n",
    "\n",
    "# printing the number of unit titles captures.        \n",
    "print(\"lenth of the list:\",len(title_lst))\n",
    "\n",
    "# printing the sample of the list of unit titles.\n",
    "title_lst[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Extracting unit synopsis\n",
    "\n",
    "Similar procedure of capturing the elements after third '\\n' line is employed to obtain synopsis. However, we will be checking for multiples of 3+1 and then append the elements to the final list.\n",
    "The code is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the list: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In this unit students consider the central production,consumption and policy contexts of popular music.The unit examines how popular music remains asignificant media and cultural industry in theproduction of content and meaning. It assesses thecore music-media output across print, broadcasting,mobile media, film, internet and related mediaindustries. The unit also looks at how governmentpolicy shapes music production and consumption, andhow local music-making and listening is shaped byglobal media practices. This includes examination ofkey debates about music-media technologies,intellectual property frameworks and the impact ofmusic across different media content.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data = open(\"pdftotext.txt\", 'r') # read the file into the variable.\n",
    "\n",
    "syn_lst=[] # inital appending list with \"\\n\" characters.\n",
    "syn_final=[] # final wrangled list\n",
    "\n",
    "# initiating a flag used to manupulate printing.\n",
    "line_check = False\n",
    "\n",
    "# initiating a count to keep track of \"\\n\" lines.\n",
    "count = 0\n",
    "\n",
    "# variable to store the current line.\n",
    "prev_line = \"\"\n",
    "\n",
    "# iterating through the complete text.\n",
    "for line in pdf_data:\n",
    "    # used to count the number of \"\\n\" lines.\n",
    "    if line == '\\n':\n",
    "        count += 1\n",
    "        \n",
    "    # when the count is multiples of 3, turn the flag on.\n",
    "    if count%3 == 1:\n",
    "        line_check = True\n",
    "    \n",
    "    # if line is noy \"\\n\" append all to a text (used to concatinate and store paragraphs)\n",
    "    if line != '\\n':\n",
    "        line = ''.join([prev_line, line])\n",
    "    \n",
    "    # if the flag is on, append the line to a list\n",
    "    if line_check:\n",
    "        syn_lst.append(line)\n",
    "        line_check = False # switch off the flag\n",
    "    prev_line = line # store the current line.\n",
    "\n",
    "# iterating through the list without first 4 lines of labels\n",
    "for i in syn_lst[3:]:\n",
    "    # check for '\\n' character and remove it from previous line. \n",
    "    if i == '\\n':\n",
    "        last = re.sub(r'\\n','',last)\n",
    "        syn_final.append(last)\n",
    "    last = i\n",
    "# append the last element\n",
    "syn_final.append(re.sub(r'\\n','',i))\n",
    "\n",
    "# printing the number of unit titles captures.        \n",
    "print(\"lenth of the list:\",len(syn_final))\n",
    "\n",
    "# printing the sample of the list of synopsis.\n",
    "syn_final[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Extracting unit outcomes\n",
    "\n",
    "Similar procedure of capturing the elements after third '\\n' line is employed to obtain outcome. However, we will be checking for multiples of 3+2 and then append the elements to the final list.\n",
    "The code is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the list: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"['discuss key media studies, popular music andcultural studies theories associated with popularmusic activity;', 'assess how popular music operatesas part of local and global media industries;', 'criticallyand independently engage with key debates andissues within the popular music industries;', 'engagewith music industry stakeholders;', 'explain andanalyse course concepts and debates in written andoral forms, and undertake independent research.']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_data = open(\"pdftotext.txt\", 'r')\n",
    "\n",
    "outcome_lst=[] # inital appending list with \"\\n\" characters.\n",
    "outcome_final=[] # final wrangled list\n",
    "\n",
    "# initiating a flag used to manupulate printing.\n",
    "line_check = False \n",
    "\n",
    "# initiating a count to keep track of \"\\n\" lines.\n",
    "count = 0\n",
    "\n",
    "# variable to store the previous line.\n",
    "prev_line = \"\"\n",
    "\n",
    "# iterating through the complete text.\n",
    "for line in pdf_data:\n",
    "    # used to count the number of \"\\n\" lines.\n",
    "    if line == '\\n':\n",
    "        count += 1\n",
    "        \n",
    "    # when the count is multiples of 3, turn the flag on.\n",
    "    if count%3 == 2:\n",
    "        line_check = True\n",
    "        \n",
    "    # if line is noy \"\\n\" append all to a text (used to concatinate and store paragraphs)\n",
    "    if line != '\\n':\n",
    "        line = ''.join([prev_line, line])\n",
    "        \n",
    "    # if the flag is on, append the line to a list\n",
    "    if line_check:\n",
    "        outcome_lst.append(line)\n",
    "        line_check = False # switch off the flag\n",
    "    prev_line = line # store the current line.\n",
    "\n",
    "# iterating through the list without first 4 lines of labels\n",
    "for i in outcome_lst[3:]:\n",
    "    if i == '\\n':\n",
    "        # check for '\\n' character and remove it from precious line. \n",
    "        last = re.sub(r'\\n','',last)\n",
    "        outcome_final.append(last)\n",
    "    last = i\n",
    "# append the last element\n",
    "outcome_final.append(re.sub(r'\\n','',i))\n",
    "\n",
    "# printing the number of unit titles captures.        \n",
    "print(\"lenth of the list:\",len(outcome_final))\n",
    "\n",
    "# printing the sample of the list of outcomes.\n",
    "outcome_final[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Normalising the first letter of a sentence\n",
    "\n",
    "I have combined the synopsis and outcome texts to normalise the first letter of sentence. I have use a code to split the data according to \".\" or \"?\" or \"!\" and convert the first letter to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"in this unit students consider the central production,consumption and policy contexts of popular music.The unit examines how popular music remains asignificant media and cultural industry in theproduction of content and meaning. it assesses thecore music-media output across print, broadcasting,mobile media, film, internet and related mediaindustries. The unit also looks at how governmentpolicy shapes music production and consumption, andhow local music-making and listening is shaped byglobal media practices. This includes examination ofkey debates about music-media technologies,intellectual property frameworks and the impact ofmusic across different media content.'discuss key media studies, popular music andcultural studies theories associated with popularmusic activity;', 'assess how popular music operatesas part of local and global media industries;', 'criticallyand independently engage with key debates andissues within the popular music industries;', 'engagewith music industry stakeholders;', 'explain andanalyse course concepts and debates in written andoral forms, and undertake independent research.'\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatinating both the lists to a single feature called \"test\".\n",
    "text = [syn_final[i] + outcome_final[i][1:-1] for i in range(len(outcome_final))]\n",
    "\n",
    "# initialising an empty list to store the final normalised result. \n",
    "final_text=[]\n",
    "# iterating through the text list with synopsis and outcome.\n",
    "for i in text:\n",
    "    # splits \"i\" with respect to \".\" or \"?\" or \"!\" and returns list.\n",
    "    j = i.split(\".|?|!\")\n",
    "    temp=[]\n",
    "    \n",
    "    # iterating though a list of sentences\n",
    "    for k in j:\n",
    "        temp.append(k.replace(k[0],k[0].lower()))\n",
    "    \n",
    "    # appends the complete line with multiple sentences to the final list.\n",
    "    final_text.append(\".\".join(temp))\n",
    "    \n",
    "# printing a sample output\n",
    "final_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Final Unit dictionary\n",
    "\n",
    "We use zip function to stitch two list to gether to form a dictionary.\n",
    "When we convert this to a dictionary, we will lose all the duplicate unit codes. This we will be left with `194` unit information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the dict: 194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"In this unit students consider the central production,consumption and policy contexts of popular music.The unit examines how popular music remains asignificant media and cultural industry in theproduction of content and meaning. It assesses thecore music-media output across print, broadcasting,mobile media, film, internet and related mediaindustries. The unit also looks at how governmentpolicy shapes music production and consumption, andhow local music-making and listening is shaped byglobal media practices. This includes examination ofkey debates about music-media technologies,intellectual property frameworks and the impact ofmusic across different media content.'discuss key media studies, popular music andcultural studies theories associated with popularmusic activity;', 'assess how popular music operatesas part of local and global media industries;', 'criticallyand independently engage with key debates andissues within the popular music industries;', 'engagewith music industry stakeholders;', 'explain andanalyse course concepts and debates in written andoral forms, and undertake independent research.'\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dictionary with unique vales using zip function on 2 lists\n",
    "Units_pdf = dict(zip(title_lst,text))\n",
    "\n",
    "# length of the dictionary\n",
    "print(\"lenth of the dict:\",len(Units_pdf))\n",
    "\n",
    "# sample output of dict values.\n",
    "next(iter(Units_pdf.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word tokenization\n",
    "\n",
    "The tokenization function is used to tokenize the all the words in the dictionary value. The first argument given to the function is the unit title which will call the value pair of that particular key.\n",
    "regular expression for tokenising is given in the specification `\\w+(?:[-']\\w+)?`. Using the same regular expression for the function.\n",
    "\n",
    "This function will return a tuple of unit title and list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of the dict: 194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'this',\n",
       " 'unit',\n",
       " 'students',\n",
       " 'consider',\n",
       " 'the',\n",
       " 'central',\n",
       " 'production',\n",
       " 'consumption',\n",
       " 'and',\n",
       " 'policy',\n",
       " 'contexts',\n",
       " 'of',\n",
       " 'popular',\n",
       " 'music',\n",
       " 'The',\n",
       " 'unit',\n",
       " 'examines',\n",
       " 'how',\n",
       " 'popular',\n",
       " 'music',\n",
       " 'remains',\n",
       " 'asignificant',\n",
       " 'media',\n",
       " 'and',\n",
       " 'cultural',\n",
       " 'industry',\n",
       " 'in',\n",
       " 'theproduction',\n",
       " 'of',\n",
       " 'content',\n",
       " 'and',\n",
       " 'meaning',\n",
       " 'It',\n",
       " 'assesses',\n",
       " 'thecore',\n",
       " 'music-media',\n",
       " 'output',\n",
       " 'across',\n",
       " 'print',\n",
       " 'broadcasting',\n",
       " 'mobile',\n",
       " 'media',\n",
       " 'film',\n",
       " 'internet',\n",
       " 'and',\n",
       " 'related',\n",
       " 'mediaindustries',\n",
       " 'The',\n",
       " 'unit',\n",
       " 'also',\n",
       " 'looks',\n",
       " 'at',\n",
       " 'how',\n",
       " 'governmentpolicy',\n",
       " 'shapes',\n",
       " 'music',\n",
       " 'production',\n",
       " 'and',\n",
       " 'consumption',\n",
       " 'andhow',\n",
       " 'local',\n",
       " 'music-making',\n",
       " 'and',\n",
       " 'listening',\n",
       " 'is',\n",
       " 'shaped',\n",
       " 'byglobal',\n",
       " 'media',\n",
       " 'practices',\n",
       " 'This',\n",
       " 'includes',\n",
       " 'examination',\n",
       " 'ofkey',\n",
       " 'debates',\n",
       " 'about',\n",
       " 'music-media',\n",
       " 'technologies',\n",
       " 'intellectual',\n",
       " 'property',\n",
       " 'frameworks',\n",
       " 'and',\n",
       " 'the',\n",
       " 'impact',\n",
       " 'ofmusic',\n",
       " 'across',\n",
       " 'different',\n",
       " 'media',\n",
       " 'content',\n",
       " 'discuss',\n",
       " 'key',\n",
       " 'media',\n",
       " 'studies',\n",
       " 'popular',\n",
       " 'music',\n",
       " 'andcultural',\n",
       " 'studies',\n",
       " 'theories',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'popularmusic',\n",
       " 'activity',\n",
       " 'assess',\n",
       " 'how',\n",
       " 'popular',\n",
       " 'music',\n",
       " 'operatesas',\n",
       " 'part',\n",
       " 'of',\n",
       " 'local',\n",
       " 'and',\n",
       " 'global',\n",
       " 'media',\n",
       " 'industries',\n",
       " 'criticallyand',\n",
       " 'independently',\n",
       " 'engage',\n",
       " 'with',\n",
       " 'key',\n",
       " 'debates',\n",
       " 'andissues',\n",
       " 'within',\n",
       " 'the',\n",
       " 'popular',\n",
       " 'music',\n",
       " 'industries',\n",
       " 'engagewith',\n",
       " 'music',\n",
       " 'industry',\n",
       " 'stakeholders',\n",
       " 'explain',\n",
       " 'andanalyse',\n",
       " 'course',\n",
       " 'concepts',\n",
       " 'and',\n",
       " 'debates',\n",
       " 'in',\n",
       " 'written',\n",
       " 'andoral',\n",
       " 'forms',\n",
       " 'and',\n",
       " 'undertake',\n",
       " 'independent',\n",
       " 'research']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\")\n",
    "# define the function to tokenise.\n",
    "def tokenize_words(title):\n",
    "    tokenized_words = vocab_tokenizer.tokenize(Units_pdf[title])\n",
    "    return (title, tokenized_words) # return a tupel of unit title and a list of tokens\n",
    "\n",
    "# call the function to tokenise the words and create a dictionary\n",
    "text_tokenized = dict(tokenize_words(title) for title in Units_pdf.keys())\n",
    "\n",
    "# length of the dictionary\n",
    "print(\"lenth of the dict:\",len(text_tokenized))\n",
    "\n",
    "# sample output of dict values.\n",
    "next(iter(text_tokenized.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial vocab count\n",
    "Finding the ratio of different unique word to the total number of words (tokens) which is know as lexical diversity.\n",
    "This is an effective method to know out output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7074 \n",
      "Total number of tokens:  28484 \n",
      "Lexical diversity:  4.026576194515126\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(text_tokenized.values())) # all the words in the dictionary\n",
    "vocab = set(words) # set of unique words.\n",
    "lexical_diversity = len(words)/len(vocab) #ratio of different unique word to the total number of words (tokens)\n",
    "print (\"Vocabulary size: \",len(vocab),\n",
    "       \"\\nTotal number of tokens: \", len(words),\n",
    "       \"\\nLexical diversity: \", lexical_diversity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Removal of stopwords and tokens with less than 3 characters\n",
    "\n",
    "We are given a text document of all the stopwords that needs to be removed.\n",
    "1. We need to read the file into a variable.\n",
    "2. Create a list and append the contents of the stopword file to a list.\n",
    "3. Use this set of stopword list to check for text token in our dictionary.\n",
    "4. Remove all the tokens with less than 3 characters.\n",
    "5. Store the result in a same dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', \"a's\", 'able', 'about', 'above']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the text file into a variable.\n",
    "stpwords = open('stopwords_en.txt', 'r')\n",
    "\n",
    "# append the file into a list of stopwords without the \"\\n\" character.\n",
    "stpwords_lst = []\n",
    "for line in stpwords:\n",
    "    stpwords_lst.append(re.sub(r'\\n','',line))\n",
    "\n",
    "# sample output of the list fo stop words.    \n",
    "stpwords_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a set of all stopwords without repitition.\n",
    "stopwords_set = set(stpwords_lst)\n",
    "\n",
    "# iterate through out unit dictionary to check for stop words and append tokens which are not in stop words. \n",
    "for i,j in text_tokenized.items():\n",
    "    text_tokenized[i] = [each for each in j if each not in stopwords_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all the with the length less than 3.  \n",
    "for i,j in text_tokenized.items():\n",
    "    for k in j:\n",
    "        if len(k) < 3:\n",
    "            j.remove(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab after removal of stop words and tokens less than 3\n",
    "Finding the ratio of different unique word to the total number of words (tokens) which is know as lexical diversity. This is an effective method to know out output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  6820 \n",
      "Total number of tokens:  18636 \n",
      "Lexical diversity:  2.7325513196480937\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(text_tokenized.values())) # all the words in the dictionary\n",
    "vocab = set(words) # set of unique words.\n",
    "lexical_diversity = len(words)/len(vocab) #ratio of different unique word to the total number of words (tokens)\n",
    "print (\"Vocabulary size: \",len(vocab),\n",
    "       \"\\nTotal number of tokens: \", len(words),\n",
    "       \"\\nLexical diversity: \", lexical_diversity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generating Bigrams\n",
    "\n",
    "Bigrams are pairs of consecutive words. This is the reason we need to determine bigrams after removing stopwords.\n",
    "If we perform this step before stop words removal, we will arrive at a lot of stop word bigrams like \"of_the\",etc. Thus, we remove stop words and the generate bigrams.\n",
    "\n",
    "We perform the following:\n",
    "1. using a function from itertools, chain all the values in the dictionary.\n",
    "2. Using NLTK collocation to determine 200 bigrams.\n",
    "3. Use MWE tokeniser to add the bigrams to the existing vocab.\n",
    "\n",
    "Code is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18636"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using a function from itertools, chain all the values in the dictionary.\n",
    "all_words = list(chain.from_iterable(text_tokenized.values()))\n",
    "\n",
    "# printing the length of the list\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using nltk library to find bigrams.\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures() # creating an bigram instance\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
    "top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200) # Top-100 bigrams\n",
    "\n",
    "#printing the length of the list\n",
    "len(top_200_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retokenise the vocab\n",
    "\n",
    "Since we determined the top 200 bigrams, we will need to add these and retokenise our vocab list. We use MWE tokeniser to ensure that we dont split the bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using MWE tokeniser to compare and append the generated bigrams into the original vocab dictionary.\n",
    "mwetokenizer = MWETokenizer(top_200_bigrams)\n",
    "units_dict =  dict((title, mwetokenizer.tokenize(text)) for title,text in text_tokenized.items())\n",
    "\n",
    "#printing the length of the dict\n",
    "len(units_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab stat after generating 200 bigrams and retokenising\n",
    "Finding the ratio of different unique word to the total number of words (tokens) which is know as lexical diversity. This is an effective method to know out output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  6653 \n",
      "Total number of tokens:  18469 \n",
      "Lexical diversity:  2.776040883811814\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(units_dict.values())) # all the words in the dictionary\n",
    "vocab = set(words) # set of unique words.\n",
    "lexical_diversity = len(words)/len(vocab) #ratio of different unique word to the total number of words (tokens)\n",
    "print (\"Vocabulary size: \",len(vocab),\n",
    "       \"\\nTotal number of tokens: \", len(words),\n",
    "       \"\\nLexical diversity: \", lexical_diversity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stemming the tokens using Porter Stemmer\n",
    "\n",
    "Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.\n",
    "We stem to limit the look up and we only stem the words which are in lower case for our task so we dont lose information(words with capital letters).\n",
    "\n",
    "We have been asked to use porter stemmer to stem our vocab data and generte more meaningful vocab data. We use the function PorterStemmer from nltk library to perform this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the function PorterStemmer from nltk.stem\n",
    "stemmer = PorterStemmer()\n",
    "for k,v in units_dict.items(): # iterating through the dictionary\n",
    "    units_dict[k] = [stemmer.stem(token) if re.search('([a-z]+[_][a-z]+)',token)==False & token.islower() else token for token in v]\n",
    "\n",
    "#printing the length of the dict\n",
    "len(units_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab stat after stemming the tokens\n",
    "Finding the ratio of different unique word to the total number of words (tokens) which is know as lexical diversity. This is an effective method to know out output.\n",
    "\n",
    "We can notice that the vocab count has not changed, that is because we have just altered the derived words to the stem word and not altered the count of the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  6653 \n",
      "Total number of tokens:  18469 \n",
      "Lexical diversity:  2.776040883811814\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(units_dict.values())) # all the words in the dictionary\n",
    "vocab = set(words) # set of unique words.\n",
    "lexical_diversity = len(words)/len(vocab) #ratio of different unique word to the total number of words (tokens)\n",
    "print (\"Vocabulary size: \",len(vocab),\n",
    "       \"\\nTotal number of tokens: \", len(words),\n",
    "       \"\\nLexical diversity: \", lexical_diversity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Removal of rare and frequent tokens\n",
    "\n",
    "We will need to remove rare tokens which is defined by the words that occur in less than 5% of the documents/observations. This is an essential process to limit out look-up just to significant vocabs.\n",
    "\n",
    "We need to perform this process in the end so that we dont miss out on potential bigram generation earlier in the task. removing the rare/frequent tokens in the begining will reduce the chances of potential bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the frequency of each token using FreqDist  \n",
    "unique=[]\n",
    "for i,j in units_dict.items():\n",
    "    unique+=list(set(j))\n",
    "freq_words = FreqDist(unique)    \n",
    "\n",
    "# appending all the tokens which is found in less than 5% of the document\n",
    "rare_tokens=[]   \n",
    "for i,j in freq_words.items():\n",
    "    #occurance of word in less than 10 documents\n",
    "    if j < 10:\n",
    "        rare_tokens.append(i)\n",
    "# appending all the tokens which is found in greater than 95% of the document\n",
    "freq_tokens=[]   \n",
    "for i,j in freq_words.items():\n",
    "    #occurance of word in more than 190 documents\n",
    "    if j>190:\n",
    "        freq_tokens.append(i)\n",
    "\n",
    "# create a list of rare and frequent tokens\n",
    "threshold = rare_tokens + freq_tokens\n",
    "\n",
    "# removing both the rare and frequent tokens from vocab.\n",
    "for i,j in units_dict.items():\n",
    "    units_dict[i] = [each for each in j if each not in threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab stat after removing the rare and frequent tokens\n",
    "Finding the ratio of different unique word to the total number of words (tokens) which is know as lexical diversity. \n",
    "\n",
    "This is an effective method to know out output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  233 \n",
      "Total number of tokens:  6726 \n",
      "Lexical diversity:  28.86695278969957\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(units_dict.values())) # all the words in the dictionary\n",
    "vocab = set(words) # set of unique words.\n",
    "lexical_diversity = len(words)/len(vocab) #ratio of different unique word to the total number of words (tokens)\n",
    "print (\"Vocabulary size: \",len(vocab),\n",
    "       \"\\nTotal number of tokens: \", len(words),\n",
    "       \"\\nLexical diversity: \", lexical_diversity) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vectorising and building a sparse vector\n",
    "\n",
    "We will create a file `29498724_vocab.txt` to write the set of vocab with their index.\n",
    "1. We sort the set of unique vocab that we obtained in the end.\n",
    "2. create a file with write permission and write these words with index.\n",
    "\n",
    "We will also need to create a file `29498724_countVec.txt` to write the sparse matrix.\n",
    "1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file with write permission\n",
    "output_dict = open(\"29498724_vocab.txt\", 'w') \n",
    "# converting the set to list.\n",
    "vocab = list(vocab)\n",
    "\n",
    "# indexing every token\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for w in vocab:\n",
    "    vocab_dict[w] = i\n",
    "    i = i + 1\n",
    "\n",
    "# Writing it to file \n",
    "count = 0\n",
    "# iterate through the sorted vocab.\n",
    "for k,v in sorted(vocab_dict.items()):\n",
    "    output_dict.write(\"{}:{}\".format(k,count))\n",
    "    output_dict.write('\\n')\n",
    "    count += 1\n",
    "\n",
    "# close the file\n",
    "output_dict.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Sparse Matrix\n",
    "output_vector = open(\"29498724_countVec.txt\", 'w')\n",
    "#Vectorizing\n",
    "for i,j in units_dict.items():\n",
    "    output_vector.write(i +',')\n",
    "    temp = [vocab_dict[w] for w in j]    \n",
    "    for k, v in FreqDist(temp).items(): \n",
    "        output_vector.write(\"{}:{},\".format(k,v))\n",
    "    output_vector.write('\\n\\n')\n",
    "# close the file    \n",
    "output_vector.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "This task ensures the following methodology that is performed:\n",
    "\n",
    "1. Extract the information from PDF file to a TEXT file. (Used https://pdftotext.com/)\n",
    "2. Text is normalized to lowercase except the capital tokens appeared in the middle of a sentence/line\n",
    "3. Perform word tokenization using regular expression, \\w+(?:[-']\\w+)?\n",
    "4. The context-independent and context-dependent stop words are removed from the vocab. The stop words file is provided. (i.e, stopwords_en.txt)\n",
    "5. First 200 meaningful bigrams (i.e., collocations) are determined using PMI measure and included in vocab. This should be done after removal of stopwords to ensure elimination of unnecessary bigrams of stopwords.\n",
    "6. Tokens should be stemmed using the Porter stemmer. Stemming must be performed after bigrams to ensure least loss of information.\n",
    "7. Rare tokens (with the threshold set to %5) must be removed from the vocab.\n",
    "8. Find the set of all the unique vocab, index and sort them in alphabetical order.\n",
    "9. Compare each document(each unit observation) and create a Sparse Matrix.\n",
    "\n",
    "We can also compare the vocab statistics and see the changes in vocab during each stage:\n",
    "\n",
    "Initial vocab stat:\n",
    "\n",
    "Vocabulary size:  7074 <br/>\n",
    "Total number of tokens:  28484 <br/>\n",
    "Lexical diversity:  4.026576194515126<br/>\n",
    "\n",
    "\n",
    "Vocab stat after removing stopwords:\n",
    "\n",
    "Vocabulary size:  6820 <br/>\n",
    "Total number of tokens:  18636 <br/>\n",
    "Lexical diversity:  2.7325513196480937<br/>\n",
    "\n",
    "\n",
    "Vocab stat after generating 200 bigrams and re-tokenising:\n",
    "\n",
    "Vocabulary size:  6653 <br/>\n",
    "Total number of tokens:  18469 <br/>\n",
    "Lexical diversity:  2.776040883811814<br/>\n",
    "\n",
    "Vocab stat after stemming:\n",
    "\n",
    "Vocabulary size:  6653 <br/>\n",
    "Total number of tokens:  18469<br/>\n",
    "Lexical diversity:  2.776040883811814<br/>\n",
    "\n",
    "\n",
    "**Final vocab stat after removing rare tokens:**\n",
    "\n",
    "**Vocabulary size:  233**<br/>\n",
    "**Total number of tokens:  6726**<br/>\n",
    "**Lexical diversity:  28.86695278969957**<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. References\n",
    "* https://pdftotext.com/\n",
    "* https://stackoverflow.com/questions/26320697/capitalization-of-each-sentence-in-a-string-in-python-3\n",
    "* https://www.nltk.org/\n",
    "* http://www.nltk.org/howto/collocations.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
